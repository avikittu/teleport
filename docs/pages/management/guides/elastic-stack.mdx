---
title: "Monitor Audit Events with the Elastic Stack"
description: "How to configure Teleport's event handler plugin to send audit events to the Elastic Stack"
---

In this guide, we will show you how to configure Teleport's event handler plugin
to send your Teleport audit events to the Elastic Stack so you can get a better
understanding of user activity in your Teleport cluster.

The event handler plugin receives audit events from the Teleport Auth Service
and forwards them to Logstash, which stores them in Elasticsearch for
visualization and alerting in Kibana. 

## Prerequisites

(!docs/pages//includes/commercial-prereqs-tabs.mdx!)

- Either a Linux host or a Kubernetes cluster running Elasticsearch, Logstash,
  and Kibana. Logstash must be listening on a TCP port that is open to traffic
  from the Teleport Auth Service. In this guide, you will also run the event
  handler plugin on this host (or in this cluster).

{/* TODO: Add version info for ELK*/}

## Step 1/ . Set up the event handler plugin

The event handler plugin is a binary that runs independently of  your Teleport
cluster. It authenticates to your Teleport cluster in order to establish a TLS
connection with the Teleport Auth Service's gRPC endpoint, and uses that
endpoint to listen for new audit events. It also authenticates to Logstash via
mutual TLS {/* TODO: double-check the mTLS/Logstash stuff*/}.

### Install the event handler plugin

The Teleport event handler receives events from Teleport's events API and
forwards them to Logstash.

<Tabs>
  <TabItem label="Linux">

  On the host where you are running the Elastic stack, execute the following
  commands:

  ```code
  $ curl -L -O https://get.gravitational.com/teleport-event-handler-v(=teleport.plugin.version=)-linux-amd64-bin.tar.gz
  $ tar -zxvf teleport-event-handler-v(=teleport.plugin.version=)-linux-amd64-bin.tar.gz
  ```

  Move the `teleport-event-handler` binary into your `PATH`.
  </TabItem>

  <TabItem label="macOS">

  On the host where you are running the Elastic stack, execute the following
  commands:

  ```code
  $ curl -L -O https://get.gravitational.com/teleport-event-handler-v(=teleport.plugin.version=)-darwin-amd64-bin.tar.gz
  $ tar -zxvf teleport-event-handler-v(=teleport.plugin.version=)-darwin-amd64-bin.tar.gz
  ```

  We currently only build the event handler plugin for amd64 machines. If your
  macOS machine uses Apple silicon, you will need to [install
  Rosetta](https://support.apple.com/en-us/HT211861) before you can run the
  event handler plugin. You can also build from source.
  </TabItem>

  <TabItem label="Docker">

  On the host where you are running the Elastic stack, execute the following
  commands:

  ```code
  $ docker pull public.ecr.aws/gravitational/teleport-plugin-event-handler:(=teleport.plugin.version=)
  ```
  </TabItem>

  <TabItem label="Helm">
  ```code
  $ helm repo add teleport https://charts.releases.teleport.dev/
  ```
  </TabItem>
  <TabItem label="From Source">
  Ensure that you have Docker Desktop installed.


  On the host where you are running the Elastic stack, execute the following
  commands to build the plugin:

  ```code
  $ git clone https://github.com/gravitational/teleport-plugins.git --depth 1
  $ cd teleport-plugins/event-handler/build.assets
  $ make build
  ```

  You can find the compiled binary within your clone of the `teleport-plugins`
  repo, with the file path, `event-handler/build/teleport-event-handler`. Move
  this into your `PATH`.

  </TabItem>
</Tabs>

### Generate a starter config file

Generate a configuration file with placeholder values for the Teleport event
handler plugin. Later in this guide, we will edit the configuration file for
your environment.

The instructions you will follow depend on whether you are running the event
handler plugin on a Linux host or a Kubernetes cluster.

<Tabs>
<TabItem scope={["cloud"]} label="Linux Host">

On the host where you are running the event handler plugin, execute the
following command to generate a sample configuration that we will edit later.
Replace `mytenant.teleport.sh` with the DNS name of your Teleport Proxy Service
or Teleport Cloud tenant:

```code
$ teleport-event-handler configure . mytenant.teleport.sh
```

</TabItem>
<TabItem label="Helm Chart">

Run the `configure` command to generate a sample configuration. Assign
`TELEPORT_CLUSTER_ADDRESS` to the DNS name and port of your Teleport Auth
Service or Proxy Service:

```code
$ TELEPORT_CLUSTER_ADDRESS=mytenant.teleport.sh:443
$ docker run -v `pwd`:/opt/teleport-plugin -w /opt/teleport-plugin public.ecr.aws/gravitational/teleport-plugin-event-handler:(=teleport.plugin.version=) configure . ${TELEPORT_CLUSTER_ADDRESS?}
```

In order to connect to Logstash, you'll need to have the root certificate and
the client credentials available as a secret. Use the following command to
create that secret in Kubernetes:

```code
$ kubectl create secret generic teleport-event-handler-client-tls --from-file=ca.crt=ca.crt,client.crt=client.crt,client.key=client.key
```

This will pack the content of `ca.crt`, `client.crt`, and `client.key` into the
secret so the Helm chart can mount them to their appropriate path.

</TabItem>
</Tabs>

The `teleport-event-handler configure` command generates several setup files:

|File(s)|Purpose|
|---|---|
| `ca.crt` and `ca.key`| Self-signed CA certificate and private key for authenticating with Logstash|
| `server.crt` and `server.key`| Logstash server certificate and key|
| `client.crt` and `client.key`| Logstash client certificate and key, signed by the generated CA|
| `teleport-event-handler-role.yaml`| `user` and `role` resource definitions for Teleport's event handler |client.
| `fluent.conf`| Fluentd plugin configuration|


### Define RBAC resources

The `teleport-event-handler configure` command generates a file called
`teleport-event-handler-role.yaml` that defines a `teleport-event-handler` role
and a user with read-only access to the `event` API:

```yaml
kind: role
metadata:
  name: teleport-event-handler
spec:
  allow:
    rules:
      - resources: ['event', 'session']
        verbs: ['list','read']
version: v5
---
kind: user
metadata:
  name: teleport-event-handler
spec:
  roles: ['teleport-event-handler']
version: v2
```

Use `tctl` to create the role and the user:

```code
$ tctl create -f teleport-event-handler-role.yaml
# user "teleport-event-handler" has been created
# role 'teleport-event-handler' has been created
```

<Notice type="tip">

If you receive an error using `tctl` on the host where you want to run the
event-handler plugin, create the `teleport-event-handler-role.yaml` file on your
workstation, then sign in to your Teleport cluster and run the `tctl` command
locally.

</Notice>

### Enable impersonation of the event handler plugin user

In order for the event-handler plugin to forward events from your Teleport
cluster, it needs a signed identity file from the cluster's certificate
authority. The `teleport-event-handler`  user cannot request this itself, and
requires another user to **impersonate** this account in order to request a
certificate.

Create a role that enables your user to impersonate the
`teleport-event-handler` user. First, paste the following YAML document into a
file called `teleport-event-handler-impersonator.yaml`:

```yaml
kind: role
version: v5
metadata:
  name: teleport-event-handler-impersonator
spec:
  # SSH options used for user sessions
  options:
    # max_session_ttl defines the TTL (time to live) of SSH certificates
    # issued to the users with this role.
    max_session_ttl: 10h

  # allow section declares a list of resource/verb combinations that are
  # allowed for the users of this role. by default nothing is allowed.
  allow:
    impersonate:
      users: ["teleport-event-handler"]
      roles: ["teleport-event-handler"]
```

Next, create the role:

```code
$ tctl create teleport-event-handler-impersonator.yaml
```

Assign this role to the current user by running the following commands:

```code
$ TELEPORT_USER=$(tsh status -f json | jq -r '.active.username')
$ tctl get users/${TELEPORT_USER?} > out.yaml
```

Now edit `out.yaml`, adding `teleport-event-handler-impersonator` to the list of
existing roles, and update via `tctl` again:

```
$ tctl create -f out.yaml
```

Log in to your Teleport cluster again to assume the new role.

### Export the access plugin identity

Like all Teleport users, `teleport-event-handler` needs signed credentials in
order to connect to your Teleport cluster. You will use the `tctl auth sign`
command to request these credentials for your plugin. 

<ScopedBlock scope={["enterprise", "oss"]}>

The format of the credentials depends on whether you have set up your network to
give the plugin direct access to the Teleport Auth Service, or if all Teleport
clients and services connect to the Teleport Proxy Service instead.

<Tabs>
<TabItem label="Connect to the Proxy Service">


The following `tctl auth sign` command impersonates the `access-plugin` user,
generates signed credentials, and writes an identity file to the local
directory:

```code
$ tctl auth sign --user=teleport-event-handler --out=auth.pem
```

The event handler plugin listens for audit events by connecting to the Teleport
Auth Service's gRPC endpoint over TLS.

The identity file, `auth.pem`, includes both TLS and SSH credentials. Your
event handler plugin uses the SSH credentials to connect to the Proxy Service,
which establishes a reverse tunnel connection to the Auth Service. The plugin
uses this reverse tunnel, along with your TLS credentials, to connect to the
Auth Service's gRPC endpoint.

You will refer to this file later when configuring the plugin.

</TabItem>
<TabItem label="Connect to the Auth Service">

If your network allows your plugin to access the Auth Service directly, e.g.,
you are running the plugin on the Auth Service host, the plugin uses TLS
credentials to connect to the Auth Service's gRPC endpoint and listen for audit
events.

You can generate TLS credentials with the following command:

```code
$ tctl auth sign --format=tls --user=teleport-event-handler --out=auth
```

This command should result in three PEM-encoded files: `auth.crt`,
`auth.key`, and `auth.cas` (certificate, private key, and CA certs
respectively). Later, you will configure the plugin to use these credentials to
connect to the Auth Service directly.

</TabItem>
</Tabs>

</ScopedBlock>

<ScopedBlock scope="cloud">


The following `tctl auth sign` command impersonates the `teleport-event-handler`
user, generates signed credentials, and writes an identity file to the local
directory:

```code
$ tctl auth sign --user=teleport-event-handler --out=auth.pem
```

Teleport's event handler plugin listens for new and updated audit events by
connecting to the Teleport Auth Service's gRPC endpoint over TLS.

The identity file, `auth.pem`, includes both TLS and SSH credentials. Your
event handler plugin uses the SSH credentials to connect to the Proxy Service,
which establishes a reverse tunnel connection to the Auth Service. The plugin
uses this reverse tunnel, along with your TLS credentials, to connect to the
Auth Service's gRPC endpoint.

You will refer to this file later when configuring the plugin.

</ScopedBlock>

<Admonition
  title="Certificate Lifetime"
>

  By default, `tctl auth sign` produces certificates with a relatively short
  lifetime. For production deployments, you can use the `--ttl` flag to ensure a
  more practical certificate lifetime, e.g., `--ttl=8760h` to export a one-year
  certificate.

</Admonition>

## Step 2/ . Configure a Logstash pipeline

On the host or Kubernetes cluster where you are running Logstash, create a
configuration file that defines a Logstash pipeline. This pipeline will receive
logs from port `9601` and forward them to the locally running Elasticsearch
instance. 

<Tabs>
<TabItem label="Linux Host">

Create an Elasticsearch user that Logstash can use to authenticate its requests
to the Elasticsearch API. The user must have access to the Elasticsearch API as
well as the ability to create index templates. We are providing this access
using the predefined roles `logstash_system` and `ingest_admin`:

```code
$ sudo /usr/share/elasticsearch/bin/elasticsearch-users \
  useradd teleport \
  --roles logstash_system,ingest_admin
```

The `elasticsearch-users` command will prompt your for a password to assign to
the user. 

Create a file called `/etc/logstash/conf.d/teleport-audit.conf` with the
following content:

```ruby
input {
  tcp{
    mode => "server"
    port => 9601
    ssl_enable =>  true
    ssl_cert => "/home/server.crt"
    ssl_key =>  "/home/server.key"
    ssl_certificate_authorities => [
      "/home/ca.crt"
    ]
    ssl_verify => true
    ssl_key_passphrase => "PASSPHRASE"
  }
}
output {
  elasticsearch {
    user => "teleport"
    password => "ELASTICSEARCH_PASSPHRASE"
  }
}
```

Update`ssl_cert`, `ssl_key`, and `ssl_certificate_authorities` to include the
locations of the server certificate, server key, and certificate authority files
that the `teleport-event-handler configure` command generated earlier. Logstash
will authenticate client certificates against the CA file and present a signed
certificate to the Teleport event-handler plugin.

To get the value of `ssl_key_passphrase`, execute the following command on the
host where you ran `teleport-event-handler configure` earlier. We will use the
passphrase added to the `fluent.conf` file to decrypt the private TLS key:

```code
$ cat fluent.conf | grep passphrase
private_key_passphrase "ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff"
```

In your Logstash pipeline configuration, use the password you assigned your
`teleport` user for `ELASTICSEARCH_PASSPHRASE`.

Ensure that Logstash can read the TLS credentials:

```code
$ chmod +r /home/server.crt /home/ca.crt /home/server.key
```

Restart Logstash:

```code
$ sudo systemctl restart logstash
```

Make sure your Logstash pipeline started successfully by running the following
command to tail Logstash's logs:

```code
$ sudo journalctl -u logstash -f
```

The resulting logs should resemble the following:

```
Sep 02 18:06:55 ip-172-37-51-68 logstash[47234]: [2022-09-02T18:06:55,742][INFO
668 ][logstash.outputs.elasticsearch][main] Installing Elasticsearch template
669 {:name=>"ecs-logstash"}
670 Sep 02 18:06:56 ip-172-37-51-68 logstash[47234]:
[2022-09-02T18:06:56,778][INFO
671 ][logstash.javapipeline    ][main] Pipeline Java execution initialization
time
672 {"seconds"=>1.08}
673 Sep 02 18:06:57 ip-172-37-51-68 logstash[47234]:
[2022-09-02T18:06:57,235][INFO
674 ][logstash.javapipeline    ][main] Pipeline started {"pipeline.id"=>"main"}
675 Sep 02 18:06:57 ip-172-37-51-68 logstash[47234]:
[2022-09-02T18:06:57,259][INFO
676 ][logstash.inputs.tcp
677 ][main][e7882833ddffc07c23ca7b124267c666133e437230eedce5ed0b25ab454f6aaa]
678 Starting tcp input listener {:address=>"0.0.0.0:9601", :ssl_enable=>true}
```

<Admonition title="Elasticsearch SSL configuration">

These instructions assume that you have already configured Elasticsearch and
Logstash to communicate via TLS. 

If your deployment is in a sandboxed or low-security environment (e.g., a demo
environment), and your `journalctl` logs for Logstash show that Elasticsearch is
unreachable, you can disable TLS for communication between Logstash and
Elasticsearch.

Edit the file `/etc/elasticsearch/elasticsearch.yaml` to set
`xpack.security.http.ssl.enabled` to `false`, then restart Elasticsearch.

</Admonition>
</TabItem>
</Tabs>

## Step 3/ . Run the event-handler plugin

### Fill in the configuration

{/* TODO: Update the config with credentials you generated earlier*/}

### Start the plugin

{/* TODO: Do this without systemd to save space*/}


## Step 3/ . Create a data view in Kibana

{/* TODO: 
Find some good audit event types to use in the data view
https://www.elastic.co/guide/en/kibana/8.0/data-views.html 

-*/}

## Step 4/ . Create an alert

{/* TODO:
https://www.elastic.co/guide/en/kibana/current/alerting-getting-started.html
-*/}

